{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "from brave import Brave\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Job centers using Brave Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "jc_list = pd.read_excel('../Datasets/JobCenter.xlsx')\n",
    "brave = Brave('BSAarKAc1psxHA5gp3fRLtkm5n40lGC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while searching for 'KyffhäuserkreisJobcenter': 1 validation error for WebSearchApiResponse\n",
      "query.is_navigational\n",
      "  Field required [type=missing, input_value={'original': 'Kyffhäuser...le': False, 'state': ''}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.6/v/missing\n"
     ]
    }
   ],
   "source": [
    "searches = []\n",
    "\n",
    "# Adding a delay to avoid sending too many requests per second, this got me errors.\n",
    "request_delay = 1\n",
    "\n",
    "for jc in jc_list['Region']:\n",
    "    # Example Berlin Jobcenter\n",
    "    query = jc + 'Jobcenter'\n",
    "    num_results = 2\n",
    "\n",
    "    try:\n",
    "        # Perform the search request\n",
    "        search_results = brave.search(q=query, count=num_results)\n",
    "        searches.append(search_results)\n",
    "        \n",
    "    except Exception as error:\n",
    "        # Handling errors\n",
    "        print(f\"An error occurred while searching for '{query}': {error}\")\n",
    "\n",
    "    # delay between requests\n",
    "    time.sleep(request_delay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arranging the results of the search in a Dataframe for management ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Willkommen beim Jobcenter Alb-Donau - jobcente...</td>\n",
       "      <td>https://www.jobcenter-alb-donau.de/</td>\n",
       "      <td>Hinweis: Unsere Informationen richten sich an ...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jobcenter Alb-Donau - 89073 Ulm | Bundesagentu...</td>\n",
       "      <td>https://www.arbeitsagentur.de/vor-ort/jobcente...</td>\n",
       "      <td>Willkommen beim &lt;strong&gt;Jobcenter&lt;/strong&gt; &lt;st...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobcenter - Stadt Baden-Baden</td>\n",
       "      <td>https://www.baden-baden.de/buergerservice/serv...</td>\n",
       "      <td>&lt;strong&gt;Das Jobcenter ist eine gemeinsame Einr...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jobcenter Baden - Baden - 76532 Baden-Baden | ...</td>\n",
       "      <td>https://www.arbeitsagentur.de/vor-ort/jobcente...</td>\n",
       "      <td>Willkommen beim &lt;strong&gt;Jobcenter&lt;/strong&gt; &lt;st...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jobcenter | Landkreis Biberach</td>\n",
       "      <td>https://www.biberach.de/jobcenter</td>\n",
       "      <td>&lt;strong&gt;Jobcenter&lt;/strong&gt; Das &lt;strong&gt;Jobcent...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Herzlich willkommen beim Jobcenter Wartburgkreis!</td>\n",
       "      <td>https://www.jobcenter-ge.de/Jobcenter/Wartburg...</td>\n",
       "      <td>als Geschäftsführerin des &lt;strong&gt;Jobcenter&lt;/s...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Weimar - Jobcenter Weimar</td>\n",
       "      <td>https://stadt.weimar.de/stadtverwaltung/organi...</td>\n",
       "      <td>Tel.: 03643 4512 970 Fax: 03643 4512 774 Email...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Jobcenter Weimarer Land</td>\n",
       "      <td>https://www.jobcenter-weimarerland.de/</td>\n",
       "      <td>Bürgergeldbeziehende stellen Anträge ... des &lt;...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Jobcenter Weimarer Land</td>\n",
       "      <td>https://www.jobcenter-weimarerland.de/</td>\n",
       "      <td>Bürgergeldbeziehende stellen Anträge ... des &lt;...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Jobcenter Weimarer Land - 99510 Apolda | Bunde...</td>\n",
       "      <td>https://www.arbeitsagentur.de/vor-ort/jobcente...</td>\n",
       "      <td>Willkommen beim &lt;strong&gt;Jobcenter&lt;/strong&gt; &lt;st...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>812 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Willkommen beim Jobcenter Alb-Donau - jobcente...   \n",
       "1    Jobcenter Alb-Donau - 89073 Ulm | Bundesagentu...   \n",
       "2                        Jobcenter - Stadt Baden-Baden   \n",
       "3    Jobcenter Baden - Baden - 76532 Baden-Baden | ...   \n",
       "4                       Jobcenter | Landkreis Biberach   \n",
       "..                                                 ...   \n",
       "807  Herzlich willkommen beim Jobcenter Wartburgkreis!   \n",
       "808                          Weimar - Jobcenter Weimar   \n",
       "809                            Jobcenter Weimarer Land   \n",
       "810                            Jobcenter Weimarer Land   \n",
       "811  Jobcenter Weimarer Land - 99510 Apolda | Bunde...   \n",
       "\n",
       "                                                   URL  \\\n",
       "0                  https://www.jobcenter-alb-donau.de/   \n",
       "1    https://www.arbeitsagentur.de/vor-ort/jobcente...   \n",
       "2    https://www.baden-baden.de/buergerservice/serv...   \n",
       "3    https://www.arbeitsagentur.de/vor-ort/jobcente...   \n",
       "4                    https://www.biberach.de/jobcenter   \n",
       "..                                                 ...   \n",
       "807  https://www.jobcenter-ge.de/Jobcenter/Wartburg...   \n",
       "808  https://stadt.weimar.de/stadtverwaltung/organi...   \n",
       "809             https://www.jobcenter-weimarerland.de/   \n",
       "810             https://www.jobcenter-weimarerland.de/   \n",
       "811  https://www.arbeitsagentur.de/vor-ort/jobcente...   \n",
       "\n",
       "                                           Description Language  \n",
       "0    Hinweis: Unsere Informationen richten sich an ...       de  \n",
       "1    Willkommen beim <strong>Jobcenter</strong> <st...       de  \n",
       "2    <strong>Das Jobcenter ist eine gemeinsame Einr...       de  \n",
       "3    Willkommen beim <strong>Jobcenter</strong> <st...       de  \n",
       "4    <strong>Jobcenter</strong> Das <strong>Jobcent...       de  \n",
       "..                                                 ...      ...  \n",
       "807  als Geschäftsführerin des <strong>Jobcenter</s...       de  \n",
       "808  Tel.: 03643 4512 970 Fax: 03643 4512 774 Email...       de  \n",
       "809  Bürgergeldbeziehende stellen Anträge ... des <...       de  \n",
       "810  Bürgergeldbeziehende stellen Anträge ... des <...       de  \n",
       "811  Willkommen beim <strong>Jobcenter</strong> <st...       de  \n",
       "\n",
       "[812 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results_list = []\n",
    "for search in searches:\n",
    "\n",
    "    # Iterate over each web result in the search\n",
    "    for web_result in search.web_results:\n",
    "\n",
    "        # Extract info from Brave searches objects\n",
    "        title = web_result['title']\n",
    "        url = web_result['url']\n",
    "        description = web_result['description']\n",
    "        language = web_result['language']\n",
    "        \n",
    "        # Append the information to the list as a dictionary\n",
    "        search_results_list.append({\n",
    "            'Title': title,\n",
    "            'URL': url,\n",
    "            'Description': description,\n",
    "            'Language': language,\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "search_results_df = pd.DataFrame(search_results_list)\n",
    "\n",
    "search_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for going to an URL and search for downloadable files inside the domain, the function is made so that it doesn't visit the same page twice\n",
    "\n",
    "This function was made by ChatGPT4, as the answer for a custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_downloadable_files(url, file_types=None, visited=None, max_depth=3, current_depth=0):\n",
    "    if file_types is None:\n",
    "        file_types = ['.pdf','.docx']\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "    \n",
    "    # Avoid revisiting the same page\n",
    "    if url in visited or current_depth > max_depth:\n",
    "        return []\n",
    "    visited.add(url)\n",
    "    \n",
    "    # Make a request to the URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        return []  # Return an empty list in case of request errors\n",
    "\n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize an empty list to store downloadable files\n",
    "    downloadable_files = []\n",
    "\n",
    "    # Find all links in the page\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            full_url = urljoin(url, href)\n",
    "            parsed_url = urlparse(full_url)\n",
    "            # Check if the link is within the same domain\n",
    "            if parsed_url.netloc != urlparse(url).netloc:\n",
    "                continue  # Skip links from different domains\n",
    "            # Check if the link is a downloadable file\n",
    "            if any(full_url.endswith(file_type) for file_type in file_types):\n",
    "                downloadable_files.append(full_url)\n",
    "            # Recursive call to follow links\n",
    "            elif current_depth < max_depth:\n",
    "                downloadable_files += find_downloadable_files(full_url, file_types, visited, max_depth, current_depth+1)\n",
    "\n",
    "    return downloadable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am currently organizing all the code, to present it in the repository. I left this script running a whole night and it only got to 30 percent, where it seemed to get stuck. It took 9 hours in total to reach this point.\n",
    "\n",
    "<img src=\"images/proof.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 2/812 [02:56<16:37:45, 73.91s/URL] /Users/diegoportillaamarillas/opt/anaconda3/envs/Unstructured10/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "Processing URLs:   2%|▏         | 13/812 [12:29<12:47:16, 57.62s/URL]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a tqdm progress bar for the loop\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m tqdm(urls, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing URLs\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     found_files \u001b[38;5;241m=\u001b[39m \u001b[43mfind_downloadable_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert url to string explicitly and pass file_types as list\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     pdfs \u001b[38;5;241m=\u001b[39m [file_url \u001b[38;5;28;01mfor\u001b[39;00m file_url \u001b[38;5;129;01min\u001b[39;00m found_files \u001b[38;5;28;01mif\u001b[39;00m file_url\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     10\u001b[0m     downloadable_pdfs\u001b[38;5;241m.\u001b[39mextend(pdfs)\n",
      "Cell \u001b[0;32mIn[18], line 41\u001b[0m, in \u001b[0;36mfind_downloadable_files\u001b[0;34m(url, file_types, visited, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# Recursive call to follow links\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m current_depth \u001b[38;5;241m<\u001b[39m max_depth:\n\u001b[0;32m---> 41\u001b[0m             downloadable_files \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mfind_downloadable_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisited\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m downloadable_files\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mfind_downloadable_files\u001b[0;34m(url, file_types, visited, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []  \u001b[38;5;66;03m# Return an empty list in case of request errors\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Parse the content with BeautifulSoup\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Initialize an empty list to store downloadable files\u001b[39;00m\n\u001b[1;32m     23\u001b[0m downloadable_files \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/requests/models.py:928\u001b[0m, in \u001b[0;36mResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Fallback to auto-detected encoding.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 928\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapparent_encoding\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Decode unicode from given encoding.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/requests/models.py:793\u001b[0m, in \u001b[0;36mResponse.apparent_encoding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapparent_encoding\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    792\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The apparent encoding, provided by the charset_normalizer or chardet libraries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchardet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/chardet/__init__.py:49\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(byte_str, should_rename_legacy)\u001b[0m\n\u001b[1;32m     47\u001b[0m     byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(byte_str)\n\u001b[1;32m     48\u001b[0m detector \u001b[38;5;241m=\u001b[39m UniversalDetector(should_rename_legacy\u001b[38;5;241m=\u001b[39mshould_rename_legacy)\n\u001b[0;32m---> 49\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m detector\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/chardet/universaldetector.py:274\u001b[0m, in \u001b[0;36mUniversalDetector.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers\u001b[38;5;241m.\u001b[39mappend(MacRomanProber())\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prober \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_charset_probers:\n\u001b[0;32m--> 274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mprober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m ProbingState\u001b[38;5;241m.\u001b[39mFOUND_IT:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m: prober\u001b[38;5;241m.\u001b[39mcharset_name,\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m: prober\u001b[38;5;241m.\u001b[39mget_confidence(),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m\"\u001b[39m: prober\u001b[38;5;241m.\u001b[39mlanguage,\n\u001b[1;32m    279\u001b[0m         }\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/chardet/charsetgroupprober.py:70\u001b[0m, in \u001b[0;36mCharSetGroupProber.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prober\u001b[38;5;241m.\u001b[39mactive:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mprober\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/chardet/sbcharsetprober.py:97\u001b[0m, in \u001b[0;36mSingleByteCharSetProber.feed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed\u001b[39m(\u001b[38;5;28mself\u001b[39m, byte_str: Union[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProbingState:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# TODO: Make filter_international_words keep things in self.alphabet\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mkeep_ascii_letters:\n\u001b[0;32m---> 97\u001b[0m         byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_international_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         byte_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_xml_tags(byte_str)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Unstructured10/lib/python3.10/site-packages/chardet/charsetprober.py:94\u001b[0m, in \u001b[0;36mCharSetProber.filter_international_words\u001b[0;34m(buf)\u001b[0m\n\u001b[1;32m     89\u001b[0m filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# This regex expression filters out only words that have at-least one\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# international character. The word may include one marker character at\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# the end.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mINTERNATIONAL_WORDS_PATTERN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     97\u001b[0m     filtered\u001b[38;5;241m.\u001b[39mextend(word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "urls = search_results_df['URL']\n",
    "downloadable_pdfs = []\n",
    "\n",
    "# Create a tqdm progress bar for the loop\n",
    "for url in tqdm(urls, desc=\"Processing URLs\", unit=\"URL\"):\n",
    "    found_files = find_downloadable_files(str(url), ['.pdf'])  # Convert url to string explicitly and pass file_types as list\n",
    "    pdfs = [file_url for file_url in found_files if file_url.endswith('.pdf')]\n",
    "    downloadable_pdfs.extend(pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check if the PDFs are useful by opening them temporarily and checking if there are keywords inside the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function made with the help of Chat GPT4\n",
    "def contains_keywords(pdf_url, keywords):\n",
    "\n",
    "    try:\n",
    "        response = requests.get(pdf_url)\n",
    "        with open(\"temp.pdf\", \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        with open(\"temp.pdf\", \"rb\") as f:\n",
    "            reader = PyPDF2.PdfFileReader(f)\n",
    "            title = reader.getDocumentInfo().title\n",
    "            text = \"\"\n",
    "            for page_num in range(reader.numPages):\n",
    "                text += reader.getPage(page_num).extractText()\n",
    "        if any(keyword.lower() in title.lower() for keyword in keywords):\n",
    "            return True\n",
    "        if any(keyword.lower() in text.lower() for keyword in keywords):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    except Exception as error:\n",
    "        print(f\"Error processing {pdf_url}: {error}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing https://www.kreis-calw.de/eGovernment/DS-GVO Führerschein.pdf: PDF starts with '<!doc', but '%PDF-' expected\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "keywords = ['Arbeitsmarkt', 'Integrationsprogramm', 'AMIP']\n",
    "\n",
    "downloadable_pdfs = []\n",
    "\n",
    "for file_url in found_files:\n",
    "    if contains_keywords(file_url, keywords):\n",
    "        downloadable_pdfs.append(file_url)\n",
    "\n",
    "print(downloadable_pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d_pdfs.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
